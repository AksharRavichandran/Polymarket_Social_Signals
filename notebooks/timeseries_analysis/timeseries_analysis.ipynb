{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Polymarket Time Series Model Comparison\n",
        "\n",
        "This notebook evaluates multiple forecasting models on NBA market price time series and summarizes performance with metrics and plots.\n",
        "\n",
        "Models: Naive, Moving Average, ARIMA, Prophet, LSTM, Transformer (optional if dependencies available).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Paths\n",
        "TS_DATA_DIR = Path('notebooks/timeseries_analysis/data')\n",
        "MARKETS_JSONL = TS_DATA_DIR / 'nba_markets.jsonl'\n",
        "PRICES_JSONL = TS_DATA_DIR / 'nba_prices_history.jsonl'\n",
        "OUT_DIR = TS_DATA_DIR / 'analysis'\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Parameters\n",
        "TRAIN_FRAC = 0.8\n",
        "MIN_POINTS = 30\n",
        "MAX_MARKETS_EVAL = 50\n",
        "DEEP_MODEL_MAX_MARKETS = 10\n",
        "LOOKBACK = 10\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 32\n",
        "SEED = 1337\n",
        "np.random.seed(SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def load_markets():\n",
        "    rows = []\n",
        "    with MARKETS_JSONL.open('r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                rows.append(json.loads(line))\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def load_prices():\n",
        "    rows = []\n",
        "    with PRICES_JSONL.open('r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "def expand_history(price_rows):\n",
        "    expanded = []\n",
        "    for row in price_rows:\n",
        "        m_id = row.get('market_id')\n",
        "        for point in row.get('history', []):\n",
        "            if isinstance(point, dict):\n",
        "                t = point.get('t')\n",
        "                p = point.get('p')\n",
        "            elif isinstance(point, (list, tuple)) and len(point) >= 2:\n",
        "                t, p = point[0], point[1]\n",
        "            else:\n",
        "                continue\n",
        "            if t is None or p is None:\n",
        "                continue\n",
        "            expanded.append({\n",
        "                'market_id': m_id,\n",
        "                'timestamp': pd.to_datetime(t, unit='s', utc=True, errors='coerce'),\n",
        "                'price': float(p),\n",
        "            })\n",
        "    df = pd.DataFrame(expanded)\n",
        "    if df.empty:\n",
        "        return pd.DataFrame(columns=['market_id', 'timestamp', 'price'])\n",
        "    df = df.dropna(subset=['timestamp'])\n",
        "    return df.sort_values(['market_id', 'timestamp'])\n",
        "\n",
        "def build_series(df):\n",
        "    series = {}\n",
        "    for mid, g in df.groupby('market_id'):\n",
        "        g = g.sort_values('timestamp')\n",
        "        if g['timestamp'].nunique() < 2:\n",
        "            continue\n",
        "        if g['price'].nunique() == 1 and float(g['price'].iloc[0]) == 0.5:\n",
        "            continue\n",
        "        s = g.set_index('timestamp')['price'].resample('D').last().dropna()\n",
        "        if len(s) >= MIN_POINTS:\n",
        "            series[mid] = s\n",
        "    return series\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "markets_df = load_markets()\n",
        "price_rows = load_prices()\n",
        "prices_df = expand_history(price_rows)\n",
        "series_by_market = build_series(prices_df)\n",
        "\n",
        "print('Markets:', len(markets_df))\n",
        "print('Price rows:', len(price_rows))\n",
        "print('Series count:', len(series_by_market))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metrics and utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def train_test_split(series, train_frac):\n",
        "    n = len(series)\n",
        "    k = int(n * train_frac)\n",
        "    return series.iloc[:k], series.iloc[k:]\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
        "\n",
        "def mae(y_true, y_pred):\n",
        "    return float(np.mean(np.abs(y_true - y_pred)))\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "    denom = np.where(y_true == 0, np.nan, y_true)\n",
        "    return float(np.nanmean(np.abs((y_true - y_pred) / denom)))\n",
        "\n",
        "def directional_accuracy(y_true, y_pred):\n",
        "    if len(y_true) < 2:\n",
        "        return np.nan\n",
        "    true_dir = np.sign(np.diff(y_true))\n",
        "    pred_dir = np.sign(np.diff(y_pred))\n",
        "    return float(np.mean(true_dir == pred_dir))\n",
        "\n",
        "def eval_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        'rmse': rmse(y_true, y_pred),\n",
        "        'mae': mae(y_true, y_pred),\n",
        "        'mape': mape(y_true, y_pred),\n",
        "        'directional_acc': directional_accuracy(y_true, y_pred),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Grid Search (lightweight)\n",
        "We run a small grid search for MA window and ARIMA order on a subset of markets to pick reasonable defaults.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "GRID_MARKETS = 5\n",
        "MA_WINDOWS = [3, 5, 7, 10]\n",
        "ARIMA_ORDERS = [(1,1,1), (2,1,1), (1,1,2)]\n",
        "\n",
        "grid_series = list(series_by_market.items())[:GRID_MARKETS]\n",
        "\n",
        "grid_rows = []\n",
        "for mid, series in grid_series:\n",
        "    train, test = train_test_split(series, TRAIN_FRAC)\n",
        "    steps = len(test)\n",
        "    if steps < 2:\n",
        "        continue\n",
        "\n",
        "    # MA window search\n",
        "    for w in MA_WINDOWS:\n",
        "        pred = forecast_ma(train, steps, window=w)\n",
        "        metrics = eval_metrics(test.values, pred)\n",
        "        grid_rows.append({'market_id': mid, 'model': 'ma', 'param': w, **metrics})\n",
        "\n",
        "    # ARIMA order search\n",
        "    try:\n",
        "        from statsmodels.tsa.arima.model import ARIMA\n",
        "        for order in ARIMA_ORDERS:\n",
        "            try:\n",
        "                model = ARIMA(train, order=order).fit()\n",
        "                pred = model.forecast(steps=steps).values\n",
        "                metrics = eval_metrics(test.values, pred)\n",
        "                grid_rows.append({'market_id': mid, 'model': 'arima', 'param': str(order), **metrics})\n",
        "            except Exception:\n",
        "                continue\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "grid_df = pd.DataFrame(grid_rows)\n",
        "grid_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if not grid_df.empty:\n",
        "    grid_summary = grid_df.groupby(['model','param'])[['rmse','mae','mape','directional_acc']].mean()\n",
        "    grid_summary = grid_summary.sort_values('rmse')\n",
        "    grid_summary.head(10)\n",
        "else:\n",
        "    print('Grid search skipped or no results.')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def forecast_naive(train, steps):\n",
        "    return np.repeat(train.iloc[-1], steps)\n",
        "\n",
        "def forecast_ma(train, steps, window=5):\n",
        "    avg = train.tail(window).mean()\n",
        "    return np.repeat(avg, steps)\n",
        "\n",
        "def forecast_arima(train, steps):\n",
        "    try:\n",
        "        from statsmodels.tsa.arima.model import ARIMA\n",
        "        model = ARIMA(train, order=(1, 1, 1)).fit()\n",
        "        return model.forecast(steps=steps).values\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "def forecast_prophet(train, steps):\n",
        "    try:\n",
        "        try:\n",
        "            from prophet import Prophet\n",
        "        except Exception:\n",
        "            from fbprophet import Prophet\n",
        "        df = pd.DataFrame({'ds': train.index.tz_convert(None), 'y': train.values})\n",
        "        m = Prophet(daily_seasonality=True)\n",
        "        m.fit(df)\n",
        "        future = m.make_future_dataframe(periods=steps, freq='D')\n",
        "        forecast = m.predict(future)\n",
        "        return forecast['yhat'].tail(steps).values\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def make_supervised(series, lookback):\n",
        "    X, y = [], []\n",
        "    values = series.values\n",
        "    for i in range(len(values) - lookback):\n",
        "        X.append(values[i:i + lookback])\n",
        "        y.append(values[i + lookback])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def forecast_lstm(train, steps, lookback=LOOKBACK):\n",
        "    try:\n",
        "        import torch\n",
        "        import torch.nn as nn\n",
        "        X, y = make_supervised(train, lookback)\n",
        "        if len(X) < 10:\n",
        "            return None\n",
        "        X = torch.tensor(X, dtype=torch.float32).unsqueeze(-1)\n",
        "        y = torch.tensor(y, dtype=torch.float32).unsqueeze(-1)\n",
        "\n",
        "        class LSTMModel(nn.Module):\n",
        "            def __init__(self, hidden=32):\n",
        "                super().__init__()\n",
        "                self.lstm = nn.LSTM(1, hidden, batch_first=True)\n",
        "                self.fc = nn.Linear(hidden, 1)\n",
        "\n",
        "            def forward(self, x):\n",
        "                out, _ = self.lstm(x)\n",
        "                return self.fc(out[:, -1, :])\n",
        "\n",
        "        model = LSTMModel()\n",
        "        opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "        loss_fn = nn.MSELoss()\n",
        "\n",
        "        for _ in range(EPOCHS):\n",
        "            idx = torch.randperm(len(X))\n",
        "            for i in range(0, len(X), BATCH_SIZE):\n",
        "                batch = idx[i:i+BATCH_SIZE]\n",
        "                xb, yb = X[batch], y[batch]\n",
        "                opt.zero_grad()\n",
        "                pred = model(xb)\n",
        "                loss = loss_fn(pred, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "        # autoregressive forecast\n",
        "        history = train.values.tolist()\n",
        "        preds = []\n",
        "        model.eval()\n",
        "        for _ in range(steps):\n",
        "            x = torch.tensor(history[-lookback:], dtype=torch.float32).view(1, lookback, 1)\n",
        "            with torch.no_grad():\n",
        "                yhat = model(x).item()\n",
        "            preds.append(yhat)\n",
        "            history.append(yhat)\n",
        "        return np.array(preds)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def forecast_transformer(train, steps, lookback=LOOKBACK):\n",
        "    try:\n",
        "        import torch\n",
        "        import torch.nn as nn\n",
        "        X, y = make_supervised(train, lookback)\n",
        "        if len(X) < 10:\n",
        "            return None\n",
        "        X = torch.tensor(X, dtype=torch.float32).unsqueeze(-1)\n",
        "        y = torch.tensor(y, dtype=torch.float32).unsqueeze(-1)\n",
        "\n",
        "        class TransformerModel(nn.Module):\n",
        "            def __init__(self, d_model=32, nhead=4, num_layers=2):\n",
        "                super().__init__()\n",
        "                self.input_proj = nn.Linear(1, d_model)\n",
        "                enc_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
        "                self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
        "                self.fc = nn.Linear(d_model, 1)\n",
        "\n",
        "            def forward(self, x):\n",
        "                x = self.input_proj(x)\n",
        "                x = self.encoder(x)\n",
        "                return self.fc(x[:, -1, :])\n",
        "\n",
        "        model = TransformerModel()\n",
        "        opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "        loss_fn = nn.MSELoss()\n",
        "\n",
        "        for _ in range(EPOCHS):\n",
        "            idx = torch.randperm(len(X))\n",
        "            for i in range(0, len(X), BATCH_SIZE):\n",
        "                batch = idx[i:i+BATCH_SIZE]\n",
        "                xb, yb = X[batch], y[batch]\n",
        "                opt.zero_grad()\n",
        "                pred = model(xb)\n",
        "                loss = loss_fn(pred, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "        history = train.values.tolist()\n",
        "        preds = []\n",
        "        model.eval()\n",
        "        for _ in range(steps):\n",
        "            x = torch.tensor(history[-lookback:], dtype=torch.float32).view(1, lookback, 1)\n",
        "            with torch.no_grad():\n",
        "                yhat = model(x).item()\n",
        "            preds.append(yhat)\n",
        "            history.append(yhat)\n",
        "        return np.array(preds)\n",
        "    except Exception:\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batch evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "results = []\n",
        "markets = list(series_by_market.items())\n",
        "markets = sorted(markets, key=lambda x: len(x[1]), reverse=True)[:MAX_MARKETS_EVAL]\n",
        "deep_markets = set([m[0] for m in markets[:DEEP_MODEL_MAX_MARKETS]])\n",
        "\n",
        "for mid, series in markets:\n",
        "    train, test = train_test_split(series, TRAIN_FRAC)\n",
        "    steps = len(test)\n",
        "    if steps < 2:\n",
        "        continue\n",
        "\n",
        "    preds = {}\n",
        "    preds['naive'] = forecast_naive(train, steps)\n",
        "    preds['ma5'] = forecast_ma(train, steps, window=5)\n",
        "\n",
        "    arima_pred = forecast_arima(train, steps)\n",
        "    if arima_pred is not None:\n",
        "        preds['arima'] = arima_pred\n",
        "\n",
        "    prophet_pred = forecast_prophet(train, steps)\n",
        "    if prophet_pred is not None:\n",
        "        preds['prophet'] = prophet_pred\n",
        "\n",
        "    if mid in deep_markets:\n",
        "        lstm_pred = forecast_lstm(train, steps)\n",
        "        if lstm_pred is not None:\n",
        "            preds['lstm'] = lstm_pred\n",
        "        transformer_pred = forecast_transformer(train, steps)\n",
        "        if transformer_pred is not None:\n",
        "            preds['transformer'] = transformer_pred\n",
        "\n",
        "    for model, yhat in preds.items():\n",
        "        metrics = eval_metrics(test.values, yhat)\n",
        "        results.append({\n",
        "            'market_id': mid,\n",
        "            'model': model,\n",
        "            'n_train': len(train),\n",
        "            'n_test': len(test),\n",
        "            **metrics,\n",
        "        })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Summary stats by model\n",
        "summary = results_df.groupby('model')[['rmse','mae','mape','directional_acc']].mean().sort_values('rmse')\n",
        "summary\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Rank models per market and compute average rank\n",
        "ranked = results_df.copy()\n",
        "ranked['rank_rmse'] = ranked.groupby('market_id')['rmse'].rank(method='average')\n",
        "ranked['rank_mae'] = ranked.groupby('market_id')['mae'].rank(method='average')\n",
        "ranked['rank_mape'] = ranked.groupby('market_id')['mape'].rank(method='average')\n",
        "ranked['rank_dir'] = ranked.groupby('market_id')['directional_acc'].rank(ascending=False, method='average')\n",
        "rank_summary = ranked.groupby('model')[['rank_rmse','rank_mae','rank_mape','rank_dir']].mean()\n",
        "rank_summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plots\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "summary.plot(kind='bar', figsize=(10, 4))\n",
        "plt.title('Mean Metrics by Model')\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# RMSE distribution\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "for model in results_df['model'].unique():\n",
        "    vals = results_df[results_df['model'] == model]['rmse'].dropna()\n",
        "    ax.hist(vals, bins=20, alpha=0.4, label=model)\n",
        "ax.legend()\n",
        "ax.set_title('RMSE Distribution by Model')\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Save outputs\n",
        "results_df.to_csv(OUT_DIR / 'model_results.csv', index=False)\n",
        "summary.to_csv(OUT_DIR / 'model_summary.csv')\n",
        "rank_summary.to_csv(OUT_DIR / 'model_rank_summary.csv')\n",
        "print('Saved results to', OUT_DIR)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}